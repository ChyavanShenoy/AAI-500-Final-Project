{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score\n",
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import GradScaler, autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StationId</th>\n",
       "      <th>Date</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>NO</th>\n",
       "      <th>NO2</th>\n",
       "      <th>NOx</th>\n",
       "      <th>NH3</th>\n",
       "      <th>CO</th>\n",
       "      <th>SO2</th>\n",
       "      <th>O3</th>\n",
       "      <th>Benzene</th>\n",
       "      <th>Toluene</th>\n",
       "      <th>Xylene</th>\n",
       "      <th>AQI</th>\n",
       "      <th>AQI_Bucket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AP001</td>\n",
       "      <td>2017-11-24</td>\n",
       "      <td>71.36</td>\n",
       "      <td>115.75</td>\n",
       "      <td>1.75</td>\n",
       "      <td>20.65</td>\n",
       "      <td>12.40</td>\n",
       "      <td>12.19</td>\n",
       "      <td>0.10</td>\n",
       "      <td>10.76</td>\n",
       "      <td>109.26</td>\n",
       "      <td>0.17</td>\n",
       "      <td>5.92</td>\n",
       "      <td>0.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AP001</td>\n",
       "      <td>2017-11-25</td>\n",
       "      <td>81.40</td>\n",
       "      <td>124.50</td>\n",
       "      <td>1.44</td>\n",
       "      <td>20.50</td>\n",
       "      <td>12.08</td>\n",
       "      <td>10.72</td>\n",
       "      <td>0.12</td>\n",
       "      <td>15.24</td>\n",
       "      <td>127.09</td>\n",
       "      <td>0.20</td>\n",
       "      <td>6.50</td>\n",
       "      <td>0.06</td>\n",
       "      <td>184.0</td>\n",
       "      <td>Moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AP001</td>\n",
       "      <td>2017-11-26</td>\n",
       "      <td>78.32</td>\n",
       "      <td>129.06</td>\n",
       "      <td>1.26</td>\n",
       "      <td>26.00</td>\n",
       "      <td>14.85</td>\n",
       "      <td>10.28</td>\n",
       "      <td>0.14</td>\n",
       "      <td>26.96</td>\n",
       "      <td>117.44</td>\n",
       "      <td>0.22</td>\n",
       "      <td>7.95</td>\n",
       "      <td>0.08</td>\n",
       "      <td>197.0</td>\n",
       "      <td>Moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AP001</td>\n",
       "      <td>2017-11-27</td>\n",
       "      <td>88.76</td>\n",
       "      <td>135.32</td>\n",
       "      <td>6.60</td>\n",
       "      <td>30.85</td>\n",
       "      <td>21.77</td>\n",
       "      <td>12.91</td>\n",
       "      <td>0.11</td>\n",
       "      <td>33.59</td>\n",
       "      <td>111.81</td>\n",
       "      <td>0.29</td>\n",
       "      <td>7.63</td>\n",
       "      <td>0.12</td>\n",
       "      <td>198.0</td>\n",
       "      <td>Moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AP001</td>\n",
       "      <td>2017-11-28</td>\n",
       "      <td>64.18</td>\n",
       "      <td>104.09</td>\n",
       "      <td>2.56</td>\n",
       "      <td>28.07</td>\n",
       "      <td>17.01</td>\n",
       "      <td>11.42</td>\n",
       "      <td>0.09</td>\n",
       "      <td>19.00</td>\n",
       "      <td>138.18</td>\n",
       "      <td>0.17</td>\n",
       "      <td>5.02</td>\n",
       "      <td>0.07</td>\n",
       "      <td>188.0</td>\n",
       "      <td>Moderate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  StationId        Date  PM2.5    PM10    NO    NO2    NOx    NH3    CO  \\\n",
       "0     AP001  2017-11-24  71.36  115.75  1.75  20.65  12.40  12.19  0.10   \n",
       "1     AP001  2017-11-25  81.40  124.50  1.44  20.50  12.08  10.72  0.12   \n",
       "2     AP001  2017-11-26  78.32  129.06  1.26  26.00  14.85  10.28  0.14   \n",
       "3     AP001  2017-11-27  88.76  135.32  6.60  30.85  21.77  12.91  0.11   \n",
       "4     AP001  2017-11-28  64.18  104.09  2.56  28.07  17.01  11.42  0.09   \n",
       "\n",
       "     SO2      O3  Benzene  Toluene  Xylene    AQI AQI_Bucket  \n",
       "0  10.76  109.26     0.17     5.92    0.10    NaN        NaN  \n",
       "1  15.24  127.09     0.20     6.50    0.06  184.0   Moderate  \n",
       "2  26.96  117.44     0.22     7.95    0.08  197.0   Moderate  \n",
       "3  33.59  111.81     0.29     7.63    0.12  198.0   Moderate  \n",
       "4  19.00  138.18     0.17     5.02    0.07  188.0   Moderate  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('station_day.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 108035 entries, 0 to 108034\n",
      "Data columns (total 16 columns):\n",
      " #   Column      Non-Null Count   Dtype  \n",
      "---  ------      --------------   -----  \n",
      " 0   StationId   108035 non-null  object \n",
      " 1   Date        108035 non-null  object \n",
      " 2   PM2.5       86410 non-null   float64\n",
      " 3   PM10        65329 non-null   float64\n",
      " 4   NO          90929 non-null   float64\n",
      " 5   NO2         91488 non-null   float64\n",
      " 6   NOx         92535 non-null   float64\n",
      " 7   NH3         59930 non-null   float64\n",
      " 8   CO          95037 non-null   float64\n",
      " 9   SO2         82831 non-null   float64\n",
      " 10  O3          82467 non-null   float64\n",
      " 11  Benzene     76580 non-null   float64\n",
      " 12  Toluene     69333 non-null   float64\n",
      " 13  Xylene      22898 non-null   float64\n",
      " 14  AQI         87025 non-null   float64\n",
      " 15  AQI_Bucket  87025 non-null   object \n",
      "dtypes: float64(13), object(3)\n",
      "memory usage: 13.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_vars = ['PM2.5', 'PM10', 'NO2', 'CO', 'SO2', 'O3']\n",
    "target_classification = 'AQI_Bucket'\n",
    "target_regression = 'AQI'\n",
    "fix_columns = ['PM2.5', 'PM10', 'NO2', 'CO', 'SO2', 'O3', 'AQI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='mean')\n",
    "df[fix_columns] = imputer.fit_transform(df[fix_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StationId</th>\n",
       "      <th>Date</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>NO</th>\n",
       "      <th>NO2</th>\n",
       "      <th>NOx</th>\n",
       "      <th>NH3</th>\n",
       "      <th>CO</th>\n",
       "      <th>SO2</th>\n",
       "      <th>O3</th>\n",
       "      <th>Benzene</th>\n",
       "      <th>Toluene</th>\n",
       "      <th>Xylene</th>\n",
       "      <th>AQI</th>\n",
       "      <th>AQI_Bucket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AP001</td>\n",
       "      <td>2017-11-24</td>\n",
       "      <td>71.36</td>\n",
       "      <td>115.75</td>\n",
       "      <td>1.75</td>\n",
       "      <td>20.65</td>\n",
       "      <td>12.40</td>\n",
       "      <td>12.19</td>\n",
       "      <td>0.10</td>\n",
       "      <td>10.76</td>\n",
       "      <td>109.26</td>\n",
       "      <td>0.17</td>\n",
       "      <td>5.92</td>\n",
       "      <td>0.10</td>\n",
       "      <td>179.74929</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AP001</td>\n",
       "      <td>2017-11-25</td>\n",
       "      <td>81.40</td>\n",
       "      <td>124.50</td>\n",
       "      <td>1.44</td>\n",
       "      <td>20.50</td>\n",
       "      <td>12.08</td>\n",
       "      <td>10.72</td>\n",
       "      <td>0.12</td>\n",
       "      <td>15.24</td>\n",
       "      <td>127.09</td>\n",
       "      <td>0.20</td>\n",
       "      <td>6.50</td>\n",
       "      <td>0.06</td>\n",
       "      <td>184.00000</td>\n",
       "      <td>Moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AP001</td>\n",
       "      <td>2017-11-26</td>\n",
       "      <td>78.32</td>\n",
       "      <td>129.06</td>\n",
       "      <td>1.26</td>\n",
       "      <td>26.00</td>\n",
       "      <td>14.85</td>\n",
       "      <td>10.28</td>\n",
       "      <td>0.14</td>\n",
       "      <td>26.96</td>\n",
       "      <td>117.44</td>\n",
       "      <td>0.22</td>\n",
       "      <td>7.95</td>\n",
       "      <td>0.08</td>\n",
       "      <td>197.00000</td>\n",
       "      <td>Moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AP001</td>\n",
       "      <td>2017-11-27</td>\n",
       "      <td>88.76</td>\n",
       "      <td>135.32</td>\n",
       "      <td>6.60</td>\n",
       "      <td>30.85</td>\n",
       "      <td>21.77</td>\n",
       "      <td>12.91</td>\n",
       "      <td>0.11</td>\n",
       "      <td>33.59</td>\n",
       "      <td>111.81</td>\n",
       "      <td>0.29</td>\n",
       "      <td>7.63</td>\n",
       "      <td>0.12</td>\n",
       "      <td>198.00000</td>\n",
       "      <td>Moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AP001</td>\n",
       "      <td>2017-11-28</td>\n",
       "      <td>64.18</td>\n",
       "      <td>104.09</td>\n",
       "      <td>2.56</td>\n",
       "      <td>28.07</td>\n",
       "      <td>17.01</td>\n",
       "      <td>11.42</td>\n",
       "      <td>0.09</td>\n",
       "      <td>19.00</td>\n",
       "      <td>138.18</td>\n",
       "      <td>0.17</td>\n",
       "      <td>5.02</td>\n",
       "      <td>0.07</td>\n",
       "      <td>188.00000</td>\n",
       "      <td>Moderate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  StationId        Date  PM2.5    PM10    NO    NO2    NOx    NH3    CO  \\\n",
       "0     AP001  2017-11-24  71.36  115.75  1.75  20.65  12.40  12.19  0.10   \n",
       "1     AP001  2017-11-25  81.40  124.50  1.44  20.50  12.08  10.72  0.12   \n",
       "2     AP001  2017-11-26  78.32  129.06  1.26  26.00  14.85  10.28  0.14   \n",
       "3     AP001  2017-11-27  88.76  135.32  6.60  30.85  21.77  12.91  0.11   \n",
       "4     AP001  2017-11-28  64.18  104.09  2.56  28.07  17.01  11.42  0.09   \n",
       "\n",
       "     SO2      O3  Benzene  Toluene  Xylene        AQI AQI_Bucket  \n",
       "0  10.76  109.26     0.17     5.92    0.10  179.74929        NaN  \n",
       "1  15.24  127.09     0.20     6.50    0.06  184.00000   Moderate  \n",
       "2  26.96  117.44     0.22     7.95    0.08  197.00000   Moderate  \n",
       "3  33.59  111.81     0.29     7.63    0.12  198.00000   Moderate  \n",
       "4  19.00  138.18     0.17     5.02    0.07  188.00000   Moderate  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df[target_classification] = label_encoder.fit_transform(df[target_classification])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StationId</th>\n",
       "      <th>Date</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>NO</th>\n",
       "      <th>NO2</th>\n",
       "      <th>NOx</th>\n",
       "      <th>NH3</th>\n",
       "      <th>CO</th>\n",
       "      <th>SO2</th>\n",
       "      <th>O3</th>\n",
       "      <th>Benzene</th>\n",
       "      <th>Toluene</th>\n",
       "      <th>Xylene</th>\n",
       "      <th>AQI</th>\n",
       "      <th>AQI_Bucket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AP001</td>\n",
       "      <td>2017-11-24</td>\n",
       "      <td>71.36</td>\n",
       "      <td>115.75</td>\n",
       "      <td>1.75</td>\n",
       "      <td>20.65</td>\n",
       "      <td>12.40</td>\n",
       "      <td>12.19</td>\n",
       "      <td>0.10</td>\n",
       "      <td>10.76</td>\n",
       "      <td>109.26</td>\n",
       "      <td>0.17</td>\n",
       "      <td>5.92</td>\n",
       "      <td>0.10</td>\n",
       "      <td>179.74929</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AP001</td>\n",
       "      <td>2017-11-25</td>\n",
       "      <td>81.40</td>\n",
       "      <td>124.50</td>\n",
       "      <td>1.44</td>\n",
       "      <td>20.50</td>\n",
       "      <td>12.08</td>\n",
       "      <td>10.72</td>\n",
       "      <td>0.12</td>\n",
       "      <td>15.24</td>\n",
       "      <td>127.09</td>\n",
       "      <td>0.20</td>\n",
       "      <td>6.50</td>\n",
       "      <td>0.06</td>\n",
       "      <td>184.00000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AP001</td>\n",
       "      <td>2017-11-26</td>\n",
       "      <td>78.32</td>\n",
       "      <td>129.06</td>\n",
       "      <td>1.26</td>\n",
       "      <td>26.00</td>\n",
       "      <td>14.85</td>\n",
       "      <td>10.28</td>\n",
       "      <td>0.14</td>\n",
       "      <td>26.96</td>\n",
       "      <td>117.44</td>\n",
       "      <td>0.22</td>\n",
       "      <td>7.95</td>\n",
       "      <td>0.08</td>\n",
       "      <td>197.00000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AP001</td>\n",
       "      <td>2017-11-27</td>\n",
       "      <td>88.76</td>\n",
       "      <td>135.32</td>\n",
       "      <td>6.60</td>\n",
       "      <td>30.85</td>\n",
       "      <td>21.77</td>\n",
       "      <td>12.91</td>\n",
       "      <td>0.11</td>\n",
       "      <td>33.59</td>\n",
       "      <td>111.81</td>\n",
       "      <td>0.29</td>\n",
       "      <td>7.63</td>\n",
       "      <td>0.12</td>\n",
       "      <td>198.00000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AP001</td>\n",
       "      <td>2017-11-28</td>\n",
       "      <td>64.18</td>\n",
       "      <td>104.09</td>\n",
       "      <td>2.56</td>\n",
       "      <td>28.07</td>\n",
       "      <td>17.01</td>\n",
       "      <td>11.42</td>\n",
       "      <td>0.09</td>\n",
       "      <td>19.00</td>\n",
       "      <td>138.18</td>\n",
       "      <td>0.17</td>\n",
       "      <td>5.02</td>\n",
       "      <td>0.07</td>\n",
       "      <td>188.00000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  StationId        Date  PM2.5    PM10    NO    NO2    NOx    NH3    CO  \\\n",
       "0     AP001  2017-11-24  71.36  115.75  1.75  20.65  12.40  12.19  0.10   \n",
       "1     AP001  2017-11-25  81.40  124.50  1.44  20.50  12.08  10.72  0.12   \n",
       "2     AP001  2017-11-26  78.32  129.06  1.26  26.00  14.85  10.28  0.14   \n",
       "3     AP001  2017-11-27  88.76  135.32  6.60  30.85  21.77  12.91  0.11   \n",
       "4     AP001  2017-11-28  64.18  104.09  2.56  28.07  17.01  11.42  0.09   \n",
       "\n",
       "     SO2      O3  Benzene  Toluene  Xylene        AQI  AQI_Bucket  \n",
       "0  10.76  109.26     0.17     5.92    0.10  179.74929           6  \n",
       "1  15.24  127.09     0.20     6.50    0.06  184.00000           1  \n",
       "2  26.96  117.44     0.22     7.95    0.08  197.00000           1  \n",
       "3  33.59  111.81     0.29     7.63    0.12  198.00000           1  \n",
       "4  19.00  138.18     0.17     5.02    0.07  188.00000           1  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 108035 entries, 0 to 108034\n",
      "Data columns (total 16 columns):\n",
      " #   Column      Non-Null Count   Dtype  \n",
      "---  ------      --------------   -----  \n",
      " 0   StationId   108035 non-null  object \n",
      " 1   Date        108035 non-null  object \n",
      " 2   PM2.5       108035 non-null  float64\n",
      " 3   PM10        108035 non-null  float64\n",
      " 4   NO          90929 non-null   float64\n",
      " 5   NO2         108035 non-null  float64\n",
      " 6   NOx         92535 non-null   float64\n",
      " 7   NH3         59930 non-null   float64\n",
      " 8   CO          108035 non-null  float64\n",
      " 9   SO2         108035 non-null  float64\n",
      " 10  O3          108035 non-null  float64\n",
      " 11  Benzene     76580 non-null   float64\n",
      " 12  Toluene     69333 non-null   float64\n",
      " 13  Xylene      22898 non-null   float64\n",
      " 14  AQI         108035 non-null  float64\n",
      " 15  AQI_Bucket  108035 non-null  int64  \n",
      "dtypes: float64(13), int64(1), object(2)\n",
      "memory usage: 13.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[learning_vars]\n",
    "\n",
    "y_class = df[target_classification]\n",
    "y_reg = df[target_regression]\n",
    "\n",
    "y_dual = df[[target_classification, target_regression]]\n",
    "\n",
    "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(X, y_class, test_size=0.2, random_state=42)\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X, y_reg, test_size=0.2, random_state=42)\n",
    "X_train_dual, X_test_dual, y_train_dual, y_test_dual = train_test_split(X, y_dual, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_class = scaler.fit_transform(X_train_class)\n",
    "X_test_class = scaler.transform(X_test_class)\n",
    "X_train_reg = scaler.fit_transform(X_train_reg)\n",
    "X_test_reg = scaler.transform(X_test_reg)\n",
    "X_train_dual = scaler.fit_transform(X_train_dual)\n",
    "X_test_dual = scaler.transform(X_test_dual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available, using cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"GPU available, using cuda\")\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    print(\"GPU not available, using cpu\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_class_tensor = torch.tensor(X_train_class, dtype=torch.float16).to(device)\n",
    "y_train_class_tensor = torch.tensor(y_train_class.values, dtype=torch.long).to(device)\n",
    "X_train_reg_tensor = torch.tensor(X_train_reg, dtype=torch.float16).to(device)\n",
    "y_train_reg_tensor = torch.tensor(y_train_reg.values, dtype=torch.float16).to(device)\n",
    "X_train_dual_tensor = torch.tensor(X_train_dual, dtype=torch.float16).to(device)\n",
    "y_train_dual_tensor = torch.tensor(y_train_dual.values, dtype=torch.float16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_class_tensor = torch.tensor(X_test_class, dtype=torch.float16).to(device)\n",
    "y_test_class_tensor = torch.tensor(y_test_class.values, dtype=torch.long).to(device)\n",
    "X_test_reg_tensor = torch.tensor(X_test_reg, dtype=torch.float16).to(device)\n",
    "y_test_reg_tensor = torch.tensor(y_test_reg.values, dtype=torch.float16).to(device)\n",
    "X_test_dual_tensor = torch.tensor(X_test_dual, dtype=torch.float16).to(device)\n",
    "y_test_dual_tensor = torch.tensor(y_test_dual.values, dtype=torch.float16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.28      0.41      1098\n",
      "           1       0.66      0.60      0.63      5867\n",
      "           2       0.58      0.29      0.39      2338\n",
      "           3       0.61      0.71      0.66      4697\n",
      "           4       0.83      0.67      0.74      1051\n",
      "           5       0.72      0.76      0.74      2391\n",
      "           6       0.61      0.85      0.71      4165\n",
      "\n",
      "    accuracy                           0.65     21607\n",
      "   macro avg       0.68      0.60      0.61     21607\n",
      "weighted avg       0.65      0.65      0.63     21607\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic_model = LogisticRegression(max_iter=200)\n",
    "logistic_model.fit(X_train_class, y_train_class)\n",
    "y_pred_class_logistic = logistic_model.predict(X_test_class)\n",
    "print(\"Logistic Regression Classification Report:\")\n",
    "print(classification_report(y_test_class, y_pred_class_logistic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.69      0.73      1098\n",
      "           1       0.78      0.81      0.79      5867\n",
      "           2       0.66      0.63      0.65      2338\n",
      "           3       0.76      0.83      0.79      4697\n",
      "           4       0.83      0.82      0.82      1051\n",
      "           5       0.77      0.79      0.78      2391\n",
      "           6       0.97      0.86      0.91      4165\n",
      "\n",
      "    accuracy                           0.80     21607\n",
      "   macro avg       0.79      0.78      0.78     21607\n",
      "weighted avg       0.80      0.80      0.80     21607\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_classifier = RandomForestClassifier(n_estimators=50, random_state=0)\n",
    "rf_classifier.fit(X_train_class, y_train_class)\n",
    "y_pred_class_rf = rf_classifier.predict(X_test_class)\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test_class, y_pred_class_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regression Metrics:\n",
      "MSE: 1656.723178675042\n",
      "R2 Score: 0.8824298696150571\n"
     ]
    }
   ],
   "source": [
    "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_regressor.fit(X_train_reg, y_train_reg)\n",
    "y_pred_reg_rf = rf_regressor.predict(X_test_reg)\n",
    "print(\"Random Forest Regression Metrics:\")\n",
    "print(\"MSE:\", mean_squared_error(y_test_reg, y_pred_reg_rf))\n",
    "print(\"R2 Score:\", r2_score(y_test_reg, y_pred_reg_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifierTorch(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MLPClassifierTorch, self).__init__()\n",
    "        # self.fc1 = nn.Linear(input_size, 200)\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        # self.fc2 = nn.Linear(200, 100)\n",
    "        # self.fc2 = nn.Linear(100, 50)\n",
    "        # self.fc3 = nn.Linear(100, 50)\n",
    "        # self.fc3 = nn.Linear(50, 25)\n",
    "        # self.fc4 = nn.Linear(50, output_size)\n",
    "        self.fc2 = nn.Linear(50, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.softmax(self.fc2(x))\n",
    "        # x = self.relu(self.fc3(x))\n",
    "        # x = self.softmax(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train_class.shape[1]\n",
    "output_size = len(label_encoder.classes_)\n",
    "mlp_classifier_torch = MLPClassifierTorch(input_size, output_size)\n",
    "mlp_classifier_torch = mlp_classifier_torch.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "optimizer = optim.Adam(mlp_classifier_torch.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10000], Loss: 1.9294\n",
      "Epoch [20/10000], Loss: 1.9151\n",
      "Epoch [30/10000], Loss: 1.8997\n",
      "Epoch [40/10000], Loss: 1.8835\n",
      "Epoch [50/10000], Loss: 1.8668\n",
      "Epoch [60/10000], Loss: 1.8496\n",
      "Epoch [70/10000], Loss: 1.8322\n",
      "Epoch [80/10000], Loss: 1.8154\n",
      "Epoch [90/10000], Loss: 1.7999\n",
      "Epoch [100/10000], Loss: 1.7859\n",
      "Epoch [110/10000], Loss: 1.7732\n",
      "Epoch [120/10000], Loss: 1.7615\n",
      "Epoch [130/10000], Loss: 1.7504\n",
      "Epoch [140/10000], Loss: 1.7398\n",
      "Epoch [150/10000], Loss: 1.7297\n",
      "Epoch [160/10000], Loss: 1.7199\n",
      "Epoch [170/10000], Loss: 1.7104\n",
      "Epoch [180/10000], Loss: 1.7011\n",
      "Epoch [190/10000], Loss: 1.6920\n",
      "Epoch [200/10000], Loss: 1.6831\n",
      "Epoch [210/10000], Loss: 1.6747\n",
      "Epoch [220/10000], Loss: 1.6666\n",
      "Epoch [230/10000], Loss: 1.6588\n",
      "Epoch [240/10000], Loss: 1.6516\n",
      "Epoch [250/10000], Loss: 1.6448\n",
      "Epoch [260/10000], Loss: 1.6384\n",
      "Epoch [270/10000], Loss: 1.6323\n",
      "Epoch [280/10000], Loss: 1.6266\n",
      "Epoch [290/10000], Loss: 1.6212\n",
      "Epoch [300/10000], Loss: 1.6160\n",
      "Epoch [310/10000], Loss: 1.6110\n",
      "Epoch [320/10000], Loss: 1.6063\n",
      "Epoch [330/10000], Loss: 1.6018\n",
      "Epoch [340/10000], Loss: 1.5975\n",
      "Epoch [350/10000], Loss: 1.5934\n",
      "Epoch [360/10000], Loss: 1.5895\n",
      "Epoch [370/10000], Loss: 1.5858\n",
      "Epoch [380/10000], Loss: 1.5822\n",
      "Epoch [390/10000], Loss: 1.5788\n",
      "Epoch [400/10000], Loss: 1.5756\n",
      "Epoch [410/10000], Loss: 1.5725\n",
      "Epoch [420/10000], Loss: 1.5695\n",
      "Epoch [430/10000], Loss: 1.5666\n",
      "Epoch [440/10000], Loss: 1.5638\n",
      "Epoch [450/10000], Loss: 1.5611\n",
      "Epoch [460/10000], Loss: 1.5586\n",
      "Epoch [470/10000], Loss: 1.5560\n",
      "Epoch [480/10000], Loss: 1.5533\n",
      "Epoch [490/10000], Loss: 1.5510\n",
      "Epoch [500/10000], Loss: 1.5487\n",
      "Epoch [510/10000], Loss: 1.5466\n",
      "Epoch [520/10000], Loss: 1.5445\n",
      "Epoch [530/10000], Loss: 1.5425\n",
      "Epoch [540/10000], Loss: 1.5406\n",
      "Epoch [550/10000], Loss: 1.5387\n",
      "Epoch [560/10000], Loss: 1.5369\n",
      "Epoch [570/10000], Loss: 1.5352\n",
      "Epoch [580/10000], Loss: 1.5335\n",
      "Epoch [590/10000], Loss: 1.5319\n",
      "Epoch [600/10000], Loss: 1.5303\n",
      "Epoch [610/10000], Loss: 1.5288\n",
      "Epoch [620/10000], Loss: 1.5273\n",
      "Epoch [630/10000], Loss: 1.5259\n",
      "Epoch [640/10000], Loss: 1.5245\n",
      "Epoch [650/10000], Loss: 1.5232\n",
      "Epoch [660/10000], Loss: 1.5219\n",
      "Epoch [670/10000], Loss: 1.5206\n",
      "Epoch [680/10000], Loss: 1.5194\n",
      "Epoch [690/10000], Loss: 1.5183\n",
      "Epoch [700/10000], Loss: 1.5171\n",
      "Epoch [710/10000], Loss: 1.5160\n",
      "Epoch [720/10000], Loss: 1.5150\n",
      "Epoch [730/10000], Loss: 1.5139\n",
      "Epoch [740/10000], Loss: 1.5129\n",
      "Epoch [750/10000], Loss: 1.5120\n",
      "Epoch [760/10000], Loss: 1.5110\n",
      "Epoch [770/10000], Loss: 1.5101\n",
      "Epoch [780/10000], Loss: 1.5092\n",
      "Epoch [790/10000], Loss: 1.5084\n",
      "Epoch [800/10000], Loss: 1.5076\n",
      "Epoch [810/10000], Loss: 1.5068\n",
      "Epoch [820/10000], Loss: 1.5060\n",
      "Epoch [830/10000], Loss: 1.5052\n",
      "Epoch [840/10000], Loss: 1.5045\n",
      "Epoch [850/10000], Loss: 1.5038\n",
      "Epoch [860/10000], Loss: 1.5032\n",
      "Epoch [870/10000], Loss: 1.5025\n",
      "Epoch [880/10000], Loss: 1.5019\n",
      "Epoch [890/10000], Loss: 1.5012\n",
      "Epoch [900/10000], Loss: 1.5006\n",
      "Epoch [910/10000], Loss: 1.5000\n",
      "Epoch [920/10000], Loss: 1.4995\n",
      "Epoch [930/10000], Loss: 1.4989\n",
      "Epoch [940/10000], Loss: 1.4984\n",
      "Epoch [950/10000], Loss: 1.4979\n",
      "Epoch [960/10000], Loss: 1.4973\n",
      "Epoch [970/10000], Loss: 1.4968\n",
      "Epoch [980/10000], Loss: 1.4964\n",
      "Epoch [990/10000], Loss: 1.4959\n",
      "Epoch [1000/10000], Loss: 1.4954\n",
      "Epoch [1010/10000], Loss: 1.4950\n",
      "Epoch [1020/10000], Loss: 1.4945\n",
      "Epoch [1030/10000], Loss: 1.4941\n",
      "Epoch [1040/10000], Loss: 1.4937\n",
      "Epoch [1050/10000], Loss: 1.4932\n",
      "Epoch [1060/10000], Loss: 1.4928\n",
      "Epoch [1070/10000], Loss: 1.4924\n",
      "Epoch [1080/10000], Loss: 1.4920\n",
      "Epoch [1090/10000], Loss: 1.4916\n",
      "Epoch [1100/10000], Loss: 1.4912\n",
      "Epoch [1110/10000], Loss: 1.4909\n",
      "Epoch [1120/10000], Loss: 1.4905\n",
      "Epoch [1130/10000], Loss: 1.4901\n",
      "Epoch [1140/10000], Loss: 1.4897\n",
      "Epoch [1150/10000], Loss: 1.4893\n",
      "Epoch [1160/10000], Loss: 1.4890\n",
      "Epoch [1170/10000], Loss: 1.4886\n",
      "Epoch [1180/10000], Loss: 1.4883\n",
      "Epoch [1190/10000], Loss: 1.4879\n",
      "Epoch [1200/10000], Loss: 1.4876\n",
      "Epoch [1210/10000], Loss: 1.4872\n",
      "Epoch [1220/10000], Loss: 1.4869\n",
      "Epoch [1230/10000], Loss: 1.4866\n",
      "Epoch [1240/10000], Loss: 1.4862\n",
      "Epoch [1250/10000], Loss: 1.4859\n",
      "Epoch [1260/10000], Loss: 1.4856\n",
      "Epoch [1270/10000], Loss: 1.4852\n",
      "Epoch [1280/10000], Loss: 1.4849\n",
      "Epoch [1290/10000], Loss: 1.4846\n",
      "Epoch [1300/10000], Loss: 1.4843\n",
      "Epoch [1310/10000], Loss: 1.4840\n",
      "Epoch [1320/10000], Loss: 1.4837\n",
      "Epoch [1330/10000], Loss: 1.4834\n",
      "Epoch [1340/10000], Loss: 1.4831\n",
      "Epoch [1350/10000], Loss: 1.4828\n",
      "Epoch [1360/10000], Loss: 1.4825\n",
      "Epoch [1370/10000], Loss: 1.4822\n",
      "Epoch [1380/10000], Loss: 1.4820\n",
      "Epoch [1390/10000], Loss: 1.4817\n",
      "Epoch [1400/10000], Loss: 1.4814\n",
      "Epoch [1410/10000], Loss: 1.4812\n",
      "Epoch [1420/10000], Loss: 1.4809\n",
      "Epoch [1430/10000], Loss: 1.4806\n",
      "Epoch [1440/10000], Loss: 1.4804\n",
      "Epoch [1450/10000], Loss: 1.4801\n",
      "Epoch [1460/10000], Loss: 1.4799\n",
      "Epoch [1470/10000], Loss: 1.4796\n",
      "Epoch [1480/10000], Loss: 1.4794\n",
      "Epoch [1490/10000], Loss: 1.4791\n",
      "Epoch [1500/10000], Loss: 1.4788\n",
      "Epoch [1510/10000], Loss: 1.4786\n",
      "Epoch [1520/10000], Loss: 1.4783\n",
      "Epoch [1530/10000], Loss: 1.4780\n",
      "Epoch [1540/10000], Loss: 1.4776\n",
      "Epoch [1550/10000], Loss: 1.4773\n",
      "Epoch [1560/10000], Loss: 1.4771\n",
      "Epoch [1570/10000], Loss: 1.4768\n",
      "Epoch [1580/10000], Loss: 1.4765\n",
      "Epoch [1590/10000], Loss: 1.4762\n",
      "Epoch [1600/10000], Loss: 1.4759\n",
      "Epoch [1610/10000], Loss: 1.4756\n",
      "Epoch [1620/10000], Loss: 1.4753\n",
      "Epoch [1630/10000], Loss: 1.4750\n",
      "Epoch [1640/10000], Loss: 1.4747\n",
      "Epoch [1650/10000], Loss: 1.4744\n",
      "Epoch [1660/10000], Loss: 1.4741\n",
      "Epoch [1670/10000], Loss: 1.4737\n",
      "Epoch [1680/10000], Loss: 1.4734\n",
      "Epoch [1690/10000], Loss: 1.4731\n",
      "Epoch [1700/10000], Loss: 1.4728\n",
      "Epoch [1710/10000], Loss: 1.4725\n",
      "Epoch [1720/10000], Loss: 1.4722\n",
      "Epoch [1730/10000], Loss: 1.4719\n",
      "Epoch [1740/10000], Loss: 1.4716\n",
      "Epoch [1750/10000], Loss: 1.4713\n",
      "Epoch [1760/10000], Loss: 1.4710\n",
      "Epoch [1770/10000], Loss: 1.4707\n",
      "Epoch [1780/10000], Loss: 1.4704\n",
      "Epoch [1790/10000], Loss: 1.4701\n",
      "Epoch [1800/10000], Loss: 1.4699\n",
      "Epoch [1810/10000], Loss: 1.4696\n",
      "Epoch [1820/10000], Loss: 1.4693\n",
      "Epoch [1830/10000], Loss: 1.4690\n",
      "Epoch [1840/10000], Loss: 1.4687\n",
      "Epoch [1850/10000], Loss: 1.4684\n",
      "Epoch [1860/10000], Loss: 1.4681\n",
      "Epoch [1870/10000], Loss: 1.4679\n",
      "Epoch [1880/10000], Loss: 1.4676\n",
      "Epoch [1890/10000], Loss: 1.4673\n",
      "Epoch [1900/10000], Loss: 1.4670\n",
      "Epoch [1910/10000], Loss: 1.4667\n",
      "Epoch [1920/10000], Loss: 1.4665\n",
      "Epoch [1930/10000], Loss: 1.4662\n",
      "Epoch [1940/10000], Loss: 1.4659\n",
      "Epoch [1950/10000], Loss: 1.4657\n",
      "Epoch [1960/10000], Loss: 1.4654\n",
      "Epoch [1970/10000], Loss: 1.4651\n",
      "Epoch [1980/10000], Loss: 1.4649\n",
      "Epoch [1990/10000], Loss: 1.4647\n",
      "Epoch [2000/10000], Loss: 1.4644\n",
      "Epoch [2010/10000], Loss: 1.4642\n",
      "Epoch [2020/10000], Loss: 1.4639\n",
      "Epoch [2030/10000], Loss: 1.4637\n",
      "Epoch [2040/10000], Loss: 1.4635\n",
      "Epoch [2050/10000], Loss: 1.4632\n",
      "Epoch [2060/10000], Loss: 1.4630\n",
      "Epoch [2070/10000], Loss: 1.4628\n",
      "Epoch [2080/10000], Loss: 1.4626\n",
      "Epoch [2090/10000], Loss: 1.4624\n",
      "Epoch [2100/10000], Loss: 1.4622\n",
      "Epoch [2110/10000], Loss: 1.4620\n",
      "Epoch [2120/10000], Loss: 1.4618\n",
      "Epoch [2130/10000], Loss: 1.4616\n",
      "Epoch [2140/10000], Loss: 1.4614\n",
      "Epoch [2150/10000], Loss: 1.4612\n",
      "Epoch [2160/10000], Loss: 1.4610\n",
      "Epoch [2170/10000], Loss: 1.4608\n",
      "Epoch [2180/10000], Loss: 1.4606\n",
      "Epoch [2190/10000], Loss: 1.4604\n",
      "Epoch [2200/10000], Loss: 1.4602\n",
      "Epoch [2210/10000], Loss: 1.4601\n",
      "Epoch [2220/10000], Loss: 1.4599\n",
      "Epoch [2230/10000], Loss: 1.4597\n",
      "Epoch [2240/10000], Loss: 1.4596\n",
      "Epoch [2250/10000], Loss: 1.4594\n",
      "Epoch [2260/10000], Loss: 1.4592\n",
      "Epoch [2270/10000], Loss: 1.4590\n",
      "Epoch [2280/10000], Loss: 1.4589\n",
      "Epoch [2290/10000], Loss: 1.4587\n",
      "Epoch [2300/10000], Loss: 1.4585\n",
      "Epoch [2310/10000], Loss: 1.4583\n",
      "Epoch [2320/10000], Loss: 1.4581\n",
      "Epoch [2330/10000], Loss: 1.4580\n",
      "Epoch [2340/10000], Loss: 1.4578\n",
      "Epoch [2350/10000], Loss: 1.4577\n",
      "Epoch [2360/10000], Loss: 1.4575\n",
      "Epoch [2370/10000], Loss: 1.4574\n",
      "Epoch [2380/10000], Loss: 1.4572\n",
      "Epoch [2390/10000], Loss: 1.4571\n",
      "Epoch [2400/10000], Loss: 1.4569\n",
      "Epoch [2410/10000], Loss: 1.4568\n",
      "Epoch [2420/10000], Loss: 1.4566\n",
      "Epoch [2430/10000], Loss: 1.4565\n",
      "Epoch [2440/10000], Loss: 1.4563\n",
      "Epoch [2450/10000], Loss: 1.4562\n",
      "Epoch [2460/10000], Loss: 1.4561\n",
      "Epoch [2470/10000], Loss: 1.4559\n",
      "Epoch [2480/10000], Loss: 1.4558\n",
      "Epoch [2490/10000], Loss: 1.4557\n",
      "Epoch [2500/10000], Loss: 1.4555\n",
      "Epoch [2510/10000], Loss: 1.4554\n",
      "Epoch [2520/10000], Loss: 1.4553\n",
      "Epoch [2530/10000], Loss: 1.4551\n",
      "Epoch [2540/10000], Loss: 1.4550\n",
      "Epoch [2550/10000], Loss: 1.4549\n",
      "Epoch [2560/10000], Loss: 1.4547\n",
      "Epoch [2570/10000], Loss: 1.4546\n",
      "Epoch [2580/10000], Loss: 1.4545\n",
      "Epoch [2590/10000], Loss: 1.4543\n",
      "Epoch [2600/10000], Loss: 1.4542\n",
      "Epoch [2610/10000], Loss: 1.4541\n",
      "Epoch [2620/10000], Loss: 1.4539\n",
      "Epoch [2630/10000], Loss: 1.4538\n",
      "Epoch [2640/10000], Loss: 1.4537\n",
      "Epoch [2650/10000], Loss: 1.4536\n",
      "Epoch [2660/10000], Loss: 1.4534\n",
      "Epoch [2670/10000], Loss: 1.4533\n",
      "Epoch [2680/10000], Loss: 1.4532\n",
      "Epoch [2690/10000], Loss: 1.4531\n",
      "Epoch [2700/10000], Loss: 1.4530\n",
      "Epoch [2710/10000], Loss: 1.4528\n",
      "Epoch [2720/10000], Loss: 1.4527\n",
      "Epoch [2730/10000], Loss: 1.4526\n",
      "Epoch [2740/10000], Loss: 1.4525\n",
      "Epoch [2750/10000], Loss: 1.4524\n",
      "Epoch [2760/10000], Loss: 1.4522\n",
      "Epoch [2770/10000], Loss: 1.4521\n",
      "Epoch [2780/10000], Loss: 1.4520\n",
      "Epoch [2790/10000], Loss: 1.4519\n",
      "Epoch [2800/10000], Loss: 1.4518\n",
      "Epoch [2810/10000], Loss: 1.4517\n",
      "Epoch [2820/10000], Loss: 1.4515\n",
      "Epoch [2830/10000], Loss: 1.4514\n",
      "Epoch [2840/10000], Loss: 1.4513\n",
      "Epoch [2850/10000], Loss: 1.4512\n",
      "Epoch [2860/10000], Loss: 1.4511\n",
      "Epoch [2870/10000], Loss: 1.4510\n",
      "Epoch [2880/10000], Loss: 1.4509\n",
      "Epoch [2890/10000], Loss: 1.4508\n",
      "Epoch [2900/10000], Loss: 1.4507\n",
      "Epoch [2910/10000], Loss: 1.4505\n",
      "Epoch [2920/10000], Loss: 1.4504\n",
      "Epoch [2930/10000], Loss: 1.4503\n",
      "Epoch [2940/10000], Loss: 1.4502\n",
      "Epoch [2950/10000], Loss: 1.4501\n",
      "Epoch [2960/10000], Loss: 1.4500\n",
      "Epoch [2970/10000], Loss: 1.4499\n",
      "Epoch [2980/10000], Loss: 1.4498\n",
      "Epoch [2990/10000], Loss: 1.4497\n",
      "Epoch [3000/10000], Loss: 1.4496\n",
      "Epoch [3010/10000], Loss: 1.4494\n",
      "Epoch [3020/10000], Loss: 1.4493\n",
      "Epoch [3030/10000], Loss: 1.4492\n",
      "Epoch [3040/10000], Loss: 1.4491\n",
      "Epoch [3050/10000], Loss: 1.4490\n",
      "Epoch [3060/10000], Loss: 1.4489\n",
      "Epoch [3070/10000], Loss: 1.4488\n",
      "Epoch [3080/10000], Loss: 1.4487\n",
      "Epoch [3090/10000], Loss: 1.4485\n",
      "Epoch [3100/10000], Loss: 1.4484\n",
      "Epoch [3110/10000], Loss: 1.4483\n",
      "Epoch [3120/10000], Loss: 1.4482\n",
      "Epoch [3130/10000], Loss: 1.4481\n",
      "Epoch [3140/10000], Loss: 1.4480\n",
      "Epoch [3150/10000], Loss: 1.4478\n",
      "Epoch [3160/10000], Loss: 1.4477\n",
      "Epoch [3170/10000], Loss: 1.4476\n",
      "Epoch [3180/10000], Loss: 1.4474\n",
      "Epoch [3190/10000], Loss: 1.4473\n",
      "Epoch [3200/10000], Loss: 1.4472\n",
      "Epoch [3210/10000], Loss: 1.4471\n",
      "Epoch [3220/10000], Loss: 1.4470\n",
      "Epoch [3230/10000], Loss: 1.4469\n",
      "Epoch [3240/10000], Loss: 1.4467\n",
      "Epoch [3250/10000], Loss: 1.4466\n",
      "Epoch [3260/10000], Loss: 1.4465\n",
      "Epoch [3270/10000], Loss: 1.4464\n",
      "Epoch [3280/10000], Loss: 1.4463\n",
      "Epoch [3290/10000], Loss: 1.4462\n",
      "Epoch [3300/10000], Loss: 1.4461\n",
      "Epoch [3310/10000], Loss: 1.4460\n",
      "Epoch [3320/10000], Loss: 1.4459\n",
      "Epoch [3330/10000], Loss: 1.4458\n",
      "Epoch [3340/10000], Loss: 1.4457\n",
      "Epoch [3350/10000], Loss: 1.4455\n",
      "Epoch [3360/10000], Loss: 1.4454\n",
      "Epoch [3370/10000], Loss: 1.4453\n",
      "Epoch [3380/10000], Loss: 1.4452\n",
      "Epoch [3390/10000], Loss: 1.4451\n",
      "Epoch [3400/10000], Loss: 1.4450\n",
      "Epoch [3410/10000], Loss: 1.4449\n",
      "Epoch [3420/10000], Loss: 1.4448\n",
      "Epoch [3430/10000], Loss: 1.4447\n",
      "Epoch [3440/10000], Loss: 1.4446\n",
      "Epoch [3450/10000], Loss: 1.4445\n",
      "Epoch [3460/10000], Loss: 1.4444\n",
      "Epoch [3470/10000], Loss: 1.4443\n",
      "Epoch [3480/10000], Loss: 1.4442\n",
      "Epoch [3490/10000], Loss: 1.4441\n",
      "Epoch [3500/10000], Loss: 1.4439\n",
      "Epoch [3510/10000], Loss: 1.4438\n",
      "Epoch [3520/10000], Loss: 1.4437\n",
      "Epoch [3530/10000], Loss: 1.4436\n",
      "Epoch [3540/10000], Loss: 1.4435\n",
      "Epoch [3550/10000], Loss: 1.4434\n",
      "Epoch [3560/10000], Loss: 1.4433\n",
      "Epoch [3570/10000], Loss: 1.4432\n",
      "Epoch [3580/10000], Loss: 1.4431\n",
      "Epoch [3590/10000], Loss: 1.4430\n",
      "Epoch [3600/10000], Loss: 1.4429\n",
      "Epoch [3610/10000], Loss: 1.4428\n",
      "Epoch [3620/10000], Loss: 1.4427\n",
      "Epoch [3630/10000], Loss: 1.4426\n",
      "Epoch [3640/10000], Loss: 1.4424\n",
      "Epoch [3650/10000], Loss: 1.4423\n",
      "Epoch [3660/10000], Loss: 1.4422\n",
      "Epoch [3670/10000], Loss: 1.4421\n",
      "Epoch [3680/10000], Loss: 1.4420\n",
      "Epoch [3690/10000], Loss: 1.4419\n",
      "Epoch [3700/10000], Loss: 1.4418\n",
      "Epoch [3710/10000], Loss: 1.4417\n",
      "Epoch [3720/10000], Loss: 1.4416\n",
      "Epoch [3730/10000], Loss: 1.4414\n",
      "Epoch [3740/10000], Loss: 1.4413\n",
      "Epoch [3750/10000], Loss: 1.4412\n",
      "Epoch [3760/10000], Loss: 1.4411\n",
      "Epoch [3770/10000], Loss: 1.4410\n",
      "Epoch [3780/10000], Loss: 1.4409\n",
      "Epoch [3790/10000], Loss: 1.4408\n",
      "Epoch [3800/10000], Loss: 1.4407\n",
      "Epoch [3810/10000], Loss: 1.4406\n",
      "Epoch [3820/10000], Loss: 1.4405\n",
      "Epoch [3830/10000], Loss: 1.4404\n",
      "Epoch [3840/10000], Loss: 1.4403\n",
      "Epoch [3850/10000], Loss: 1.4402\n",
      "Epoch [3860/10000], Loss: 1.4401\n",
      "Epoch [3870/10000], Loss: 1.4400\n",
      "Epoch [3880/10000], Loss: 1.4399\n",
      "Epoch [3890/10000], Loss: 1.4398\n",
      "Epoch [3900/10000], Loss: 1.4397\n",
      "Epoch [3910/10000], Loss: 1.4397\n",
      "Epoch [3920/10000], Loss: 1.4396\n",
      "Epoch [3930/10000], Loss: 1.4395\n",
      "Epoch [3940/10000], Loss: 1.4394\n",
      "Epoch [3950/10000], Loss: 1.4393\n",
      "Epoch [3960/10000], Loss: 1.4392\n",
      "Epoch [3970/10000], Loss: 1.4391\n",
      "Epoch [3980/10000], Loss: 1.4391\n",
      "Epoch [3990/10000], Loss: 1.4390\n",
      "Epoch [4000/10000], Loss: 1.4389\n",
      "Epoch [4010/10000], Loss: 1.4388\n",
      "Epoch [4020/10000], Loss: 1.4387\n",
      "Epoch [4030/10000], Loss: 1.4387\n",
      "Epoch [4040/10000], Loss: 1.4386\n",
      "Epoch [4050/10000], Loss: 1.4385\n",
      "Epoch [4060/10000], Loss: 1.4384\n",
      "Epoch [4070/10000], Loss: 1.4384\n",
      "Epoch [4080/10000], Loss: 1.4383\n",
      "Epoch [4090/10000], Loss: 1.4382\n",
      "Epoch [4100/10000], Loss: 1.4381\n",
      "Epoch [4110/10000], Loss: 1.4381\n",
      "Epoch [4120/10000], Loss: 1.4380\n",
      "Epoch [4130/10000], Loss: 1.4379\n",
      "Epoch [4140/10000], Loss: 1.4378\n",
      "Epoch [4150/10000], Loss: 1.4378\n",
      "Epoch [4160/10000], Loss: 1.4377\n",
      "Epoch [4170/10000], Loss: 1.4376\n",
      "Epoch [4180/10000], Loss: 1.4375\n",
      "Epoch [4190/10000], Loss: 1.4374\n",
      "Epoch [4200/10000], Loss: 1.4374\n",
      "Epoch [4210/10000], Loss: 1.4373\n",
      "Epoch [4220/10000], Loss: 1.4372\n",
      "Epoch [4230/10000], Loss: 1.4371\n",
      "Epoch [4240/10000], Loss: 1.4371\n",
      "Epoch [4250/10000], Loss: 1.4370\n",
      "Epoch [4260/10000], Loss: 1.4369\n",
      "Epoch [4270/10000], Loss: 1.4368\n",
      "Epoch [4280/10000], Loss: 1.4367\n",
      "Epoch [4290/10000], Loss: 1.4366\n",
      "Epoch [4300/10000], Loss: 1.4366\n",
      "Epoch [4310/10000], Loss: 1.4365\n",
      "Epoch [4320/10000], Loss: 1.4364\n",
      "Epoch [4330/10000], Loss: 1.4363\n",
      "Epoch [4340/10000], Loss: 1.4363\n",
      "Epoch [4350/10000], Loss: 1.4362\n",
      "Epoch [4360/10000], Loss: 1.4361\n",
      "Epoch [4370/10000], Loss: 1.4360\n",
      "Epoch [4380/10000], Loss: 1.4360\n",
      "Epoch [4390/10000], Loss: 1.4359\n",
      "Epoch [4400/10000], Loss: 1.4358\n",
      "Epoch [4410/10000], Loss: 1.4357\n",
      "Epoch [4420/10000], Loss: 1.4357\n",
      "Epoch [4430/10000], Loss: 1.4356\n",
      "Epoch [4440/10000], Loss: 1.4355\n",
      "Epoch [4450/10000], Loss: 1.4354\n",
      "Epoch [4460/10000], Loss: 1.4353\n",
      "Epoch [4470/10000], Loss: 1.4353\n",
      "Epoch [4480/10000], Loss: 1.4352\n",
      "Epoch [4490/10000], Loss: 1.4351\n",
      "Epoch [4500/10000], Loss: 1.4350\n",
      "Epoch [4510/10000], Loss: 1.4350\n",
      "Epoch [4520/10000], Loss: 1.4349\n",
      "Epoch [4530/10000], Loss: 1.4348\n",
      "Epoch [4540/10000], Loss: 1.4348\n",
      "Epoch [4550/10000], Loss: 1.4347\n",
      "Epoch [4560/10000], Loss: 1.4346\n",
      "Epoch [4570/10000], Loss: 1.4345\n",
      "Epoch [4580/10000], Loss: 1.4345\n",
      "Epoch [4590/10000], Loss: 1.4344\n",
      "Epoch [4600/10000], Loss: 1.4343\n",
      "Epoch [4610/10000], Loss: 1.4343\n",
      "Epoch [4620/10000], Loss: 1.4342\n",
      "Epoch [4630/10000], Loss: 1.4341\n",
      "Epoch [4640/10000], Loss: 1.4340\n",
      "Epoch [4650/10000], Loss: 1.4340\n",
      "Epoch [4660/10000], Loss: 1.4339\n",
      "Epoch [4670/10000], Loss: 1.4338\n",
      "Epoch [4680/10000], Loss: 1.4338\n",
      "Epoch [4690/10000], Loss: 1.4337\n",
      "Epoch [4700/10000], Loss: 1.4336\n",
      "Epoch [4710/10000], Loss: 1.4336\n",
      "Epoch [4720/10000], Loss: 1.4335\n",
      "Epoch [4730/10000], Loss: 1.4334\n",
      "Epoch [4740/10000], Loss: 1.4334\n",
      "Epoch [4750/10000], Loss: 1.4333\n",
      "Epoch [4760/10000], Loss: 1.4332\n",
      "Epoch [4770/10000], Loss: 1.4331\n",
      "Epoch [4780/10000], Loss: 1.4331\n",
      "Epoch [4790/10000], Loss: 1.4330\n",
      "Epoch [4800/10000], Loss: 1.4329\n",
      "Epoch [4810/10000], Loss: 1.4328\n",
      "Epoch [4820/10000], Loss: 1.4328\n",
      "Epoch [4830/10000], Loss: 1.4327\n",
      "Epoch [4840/10000], Loss: 1.4326\n",
      "Epoch [4850/10000], Loss: 1.4325\n",
      "Epoch [4860/10000], Loss: 1.4325\n",
      "Epoch [4870/10000], Loss: 1.4324\n",
      "Epoch [4880/10000], Loss: 1.4323\n",
      "Epoch [4890/10000], Loss: 1.4322\n",
      "Epoch [4900/10000], Loss: 1.4322\n",
      "Epoch [4910/10000], Loss: 1.4321\n",
      "Epoch [4920/10000], Loss: 1.4320\n",
      "Epoch [4930/10000], Loss: 1.4319\n",
      "Epoch [4940/10000], Loss: 1.4319\n",
      "Epoch [4950/10000], Loss: 1.4318\n",
      "Epoch [4960/10000], Loss: 1.4317\n",
      "Epoch [4970/10000], Loss: 1.4316\n",
      "Epoch [4980/10000], Loss: 1.4316\n",
      "Epoch [4990/10000], Loss: 1.4315\n",
      "Epoch [5000/10000], Loss: 1.4314\n",
      "Epoch [5010/10000], Loss: 1.4313\n",
      "Epoch [5020/10000], Loss: 1.4312\n",
      "Epoch [5030/10000], Loss: 1.4312\n",
      "Epoch [5040/10000], Loss: 1.4311\n",
      "Epoch [5050/10000], Loss: 1.4310\n",
      "Epoch [5060/10000], Loss: 1.4310\n",
      "Epoch [5070/10000], Loss: 1.4309\n",
      "Epoch [5080/10000], Loss: 1.4308\n",
      "Epoch [5090/10000], Loss: 1.4307\n",
      "Epoch [5100/10000], Loss: 1.4306\n",
      "Epoch [5110/10000], Loss: 1.4305\n",
      "Epoch [5120/10000], Loss: 1.4305\n",
      "Epoch [5130/10000], Loss: 1.4304\n",
      "Epoch [5140/10000], Loss: 1.4303\n",
      "Epoch [5150/10000], Loss: 1.4301\n",
      "Epoch [5160/10000], Loss: 1.4300\n",
      "Epoch [5170/10000], Loss: 1.4299\n",
      "Epoch [5180/10000], Loss: 1.4297\n",
      "Epoch [5190/10000], Loss: 1.4295\n",
      "Epoch [5200/10000], Loss: 1.4292\n",
      "Epoch [5210/10000], Loss: 1.4289\n",
      "Epoch [5220/10000], Loss: 1.4287\n",
      "Epoch [5230/10000], Loss: 1.4285\n",
      "Epoch [5240/10000], Loss: 1.4283\n",
      "Epoch [5250/10000], Loss: 1.4282\n",
      "Epoch [5260/10000], Loss: 1.4280\n",
      "Epoch [5270/10000], Loss: 1.4279\n",
      "Epoch [5280/10000], Loss: 1.4278\n",
      "Epoch [5290/10000], Loss: 1.4277\n",
      "Epoch [5300/10000], Loss: 1.4276\n",
      "Epoch [5310/10000], Loss: 1.4275\n",
      "Epoch [5320/10000], Loss: 1.4274\n",
      "Epoch [5330/10000], Loss: 1.4273\n",
      "Epoch [5340/10000], Loss: 1.4271\n",
      "Epoch [5350/10000], Loss: 1.4270\n",
      "Epoch [5360/10000], Loss: 1.4269\n",
      "Epoch [5370/10000], Loss: 1.4268\n",
      "Epoch [5380/10000], Loss: 1.4267\n",
      "Epoch [5390/10000], Loss: 1.4266\n",
      "Epoch [5400/10000], Loss: 1.4265\n",
      "Epoch [5410/10000], Loss: 1.4264\n",
      "Epoch [5420/10000], Loss: 1.4263\n",
      "Epoch [5430/10000], Loss: 1.4262\n",
      "Epoch [5440/10000], Loss: 1.4261\n",
      "Epoch [5450/10000], Loss: 1.4260\n",
      "Epoch [5460/10000], Loss: 1.4259\n",
      "Epoch [5470/10000], Loss: 1.4258\n",
      "Epoch [5480/10000], Loss: 1.4257\n",
      "Epoch [5490/10000], Loss: 1.4256\n",
      "Epoch [5500/10000], Loss: 1.4255\n",
      "Epoch [5510/10000], Loss: 1.4254\n",
      "Epoch [5520/10000], Loss: 1.4253\n",
      "Epoch [5530/10000], Loss: 1.4252\n",
      "Epoch [5540/10000], Loss: 1.4251\n",
      "Epoch [5550/10000], Loss: 1.4250\n",
      "Epoch [5560/10000], Loss: 1.4249\n",
      "Epoch [5570/10000], Loss: 1.4248\n",
      "Epoch [5580/10000], Loss: 1.4247\n",
      "Epoch [5590/10000], Loss: 1.4245\n",
      "Epoch [5600/10000], Loss: 1.4244\n",
      "Epoch [5610/10000], Loss: 1.4243\n",
      "Epoch [5620/10000], Loss: 1.4242\n",
      "Epoch [5630/10000], Loss: 1.4241\n",
      "Epoch [5640/10000], Loss: 1.4241\n",
      "Epoch [5650/10000], Loss: 1.4240\n",
      "Epoch [5660/10000], Loss: 1.4239\n",
      "Epoch [5670/10000], Loss: 1.4238\n",
      "Epoch [5680/10000], Loss: 1.4237\n",
      "Epoch [5690/10000], Loss: 1.4236\n",
      "Epoch [5700/10000], Loss: 1.4235\n",
      "Epoch [5710/10000], Loss: 1.4234\n",
      "Epoch [5720/10000], Loss: 1.4234\n",
      "Epoch [5730/10000], Loss: 1.4233\n",
      "Epoch [5740/10000], Loss: 1.4232\n",
      "Epoch [5750/10000], Loss: 1.4231\n",
      "Epoch [5760/10000], Loss: 1.4230\n",
      "Epoch [5770/10000], Loss: 1.4229\n",
      "Epoch [5780/10000], Loss: 1.4228\n",
      "Epoch [5790/10000], Loss: 1.4227\n",
      "Epoch [5800/10000], Loss: 1.4226\n",
      "Epoch [5810/10000], Loss: 1.4225\n",
      "Epoch [5820/10000], Loss: 1.4224\n",
      "Epoch [5830/10000], Loss: 1.4223\n",
      "Epoch [5840/10000], Loss: 1.4221\n",
      "Epoch [5850/10000], Loss: 1.4220\n",
      "Epoch [5860/10000], Loss: 1.4219\n",
      "Epoch [5870/10000], Loss: 1.4218\n",
      "Epoch [5880/10000], Loss: 1.4217\n",
      "Epoch [5890/10000], Loss: 1.4216\n",
      "Epoch [5900/10000], Loss: 1.4215\n",
      "Epoch [5910/10000], Loss: 1.4214\n",
      "Epoch [5920/10000], Loss: 1.4213\n",
      "Epoch [5930/10000], Loss: 1.4212\n",
      "Epoch [5940/10000], Loss: 1.4210\n",
      "Epoch [5950/10000], Loss: 1.4208\n",
      "Epoch [5960/10000], Loss: 1.4206\n",
      "Epoch [5970/10000], Loss: 1.4204\n",
      "Epoch [5980/10000], Loss: 1.4201\n",
      "Epoch [5990/10000], Loss: 1.4199\n",
      "Epoch [6000/10000], Loss: 1.4197\n",
      "Epoch [6010/10000], Loss: 1.4195\n",
      "Epoch [6020/10000], Loss: 1.4194\n",
      "Epoch [6030/10000], Loss: 1.4192\n",
      "Epoch [6040/10000], Loss: 1.4190\n",
      "Epoch [6050/10000], Loss: 1.4189\n",
      "Epoch [6060/10000], Loss: 1.4187\n",
      "Epoch [6070/10000], Loss: 1.4186\n",
      "Epoch [6080/10000], Loss: 1.4184\n",
      "Epoch [6090/10000], Loss: 1.4183\n",
      "Epoch [6100/10000], Loss: 1.4181\n",
      "Epoch [6110/10000], Loss: 1.4180\n",
      "Epoch [6120/10000], Loss: 1.4179\n",
      "Epoch [6130/10000], Loss: 1.4178\n",
      "Epoch [6140/10000], Loss: 1.4176\n",
      "Epoch [6150/10000], Loss: 1.4175\n",
      "Epoch [6160/10000], Loss: 1.4174\n",
      "Epoch [6170/10000], Loss: 1.4173\n",
      "Epoch [6180/10000], Loss: 1.4171\n",
      "Epoch [6190/10000], Loss: 1.4170\n",
      "Epoch [6200/10000], Loss: 1.4169\n",
      "Epoch [6210/10000], Loss: 1.4168\n",
      "Epoch [6220/10000], Loss: 1.4167\n",
      "Epoch [6230/10000], Loss: 1.4165\n",
      "Epoch [6240/10000], Loss: 1.4164\n",
      "Epoch [6250/10000], Loss: 1.4163\n",
      "Epoch [6260/10000], Loss: 1.4162\n",
      "Epoch [6270/10000], Loss: 1.4160\n",
      "Epoch [6280/10000], Loss: 1.4159\n",
      "Epoch [6290/10000], Loss: 1.4158\n",
      "Epoch [6300/10000], Loss: 1.4156\n",
      "Epoch [6310/10000], Loss: 1.4155\n",
      "Epoch [6320/10000], Loss: 1.4154\n",
      "Epoch [6330/10000], Loss: 1.4152\n",
      "Epoch [6340/10000], Loss: 1.4151\n",
      "Epoch [6350/10000], Loss: 1.4149\n",
      "Epoch [6360/10000], Loss: 1.4148\n",
      "Epoch [6370/10000], Loss: 1.4147\n",
      "Epoch [6380/10000], Loss: 1.4145\n",
      "Epoch [6390/10000], Loss: 1.4144\n",
      "Epoch [6400/10000], Loss: 1.4143\n",
      "Epoch [6410/10000], Loss: 1.4142\n",
      "Epoch [6420/10000], Loss: 1.4141\n",
      "Epoch [6430/10000], Loss: 1.4139\n",
      "Epoch [6440/10000], Loss: 1.4138\n",
      "Epoch [6450/10000], Loss: 1.4137\n",
      "Epoch [6460/10000], Loss: 1.4135\n",
      "Epoch [6470/10000], Loss: 1.4134\n",
      "Epoch [6480/10000], Loss: 1.4132\n",
      "Epoch [6490/10000], Loss: 1.4131\n",
      "Epoch [6500/10000], Loss: 1.4129\n",
      "Epoch [6510/10000], Loss: 1.4127\n",
      "Epoch [6520/10000], Loss: 1.4125\n",
      "Epoch [6530/10000], Loss: 1.4123\n",
      "Epoch [6540/10000], Loss: 1.4122\n",
      "Epoch [6550/10000], Loss: 1.4121\n",
      "Epoch [6560/10000], Loss: 1.4120\n",
      "Epoch [6570/10000], Loss: 1.4119\n",
      "Epoch [6580/10000], Loss: 1.4118\n",
      "Epoch [6590/10000], Loss: 1.4117\n",
      "Epoch [6600/10000], Loss: 1.4116\n",
      "Epoch [6610/10000], Loss: 1.4115\n",
      "Epoch [6620/10000], Loss: 1.4114\n",
      "Epoch [6630/10000], Loss: 1.4113\n",
      "Epoch [6640/10000], Loss: 1.4112\n",
      "Epoch [6650/10000], Loss: 1.4111\n",
      "Epoch [6660/10000], Loss: 1.4110\n",
      "Epoch [6670/10000], Loss: 1.4109\n",
      "Epoch [6680/10000], Loss: 1.4108\n",
      "Epoch [6690/10000], Loss: 1.4107\n",
      "Epoch [6700/10000], Loss: 1.4106\n",
      "Epoch [6710/10000], Loss: 1.4105\n",
      "Epoch [6720/10000], Loss: 1.4104\n",
      "Epoch [6730/10000], Loss: 1.4103\n",
      "Epoch [6740/10000], Loss: 1.4103\n",
      "Epoch [6750/10000], Loss: 1.4102\n",
      "Epoch [6760/10000], Loss: 1.4101\n",
      "Epoch [6770/10000], Loss: 1.4100\n",
      "Epoch [6780/10000], Loss: 1.4099\n",
      "Epoch [6790/10000], Loss: 1.4098\n",
      "Epoch [6800/10000], Loss: 1.4097\n",
      "Epoch [6810/10000], Loss: 1.4096\n",
      "Epoch [6820/10000], Loss: 1.4095\n",
      "Epoch [6830/10000], Loss: 1.4094\n",
      "Epoch [6840/10000], Loss: 1.4093\n",
      "Epoch [6850/10000], Loss: 1.4093\n",
      "Epoch [6860/10000], Loss: 1.4092\n",
      "Epoch [6870/10000], Loss: 1.4091\n",
      "Epoch [6880/10000], Loss: 1.4091\n",
      "Epoch [6890/10000], Loss: 1.4090\n",
      "Epoch [6900/10000], Loss: 1.4089\n",
      "Epoch [6910/10000], Loss: 1.4089\n",
      "Epoch [6920/10000], Loss: 1.4088\n",
      "Epoch [6930/10000], Loss: 1.4088\n",
      "Epoch [6940/10000], Loss: 1.4087\n",
      "Epoch [6950/10000], Loss: 1.4086\n",
      "Epoch [6960/10000], Loss: 1.4086\n",
      "Epoch [6970/10000], Loss: 1.4085\n",
      "Epoch [6980/10000], Loss: 1.4085\n",
      "Epoch [6990/10000], Loss: 1.4084\n",
      "Epoch [7000/10000], Loss: 1.4083\n",
      "Epoch [7010/10000], Loss: 1.4083\n",
      "Epoch [7020/10000], Loss: 1.4082\n",
      "Epoch [7030/10000], Loss: 1.4082\n",
      "Epoch [7040/10000], Loss: 1.4081\n",
      "Epoch [7050/10000], Loss: 1.4081\n",
      "Epoch [7060/10000], Loss: 1.4080\n",
      "Epoch [7070/10000], Loss: 1.4080\n",
      "Epoch [7080/10000], Loss: 1.4079\n",
      "Epoch [7090/10000], Loss: 1.4079\n",
      "Epoch [7100/10000], Loss: 1.4078\n",
      "Epoch [7110/10000], Loss: 1.4078\n",
      "Epoch [7120/10000], Loss: 1.4077\n",
      "Epoch [7130/10000], Loss: 1.4077\n",
      "Epoch [7140/10000], Loss: 1.4076\n",
      "Epoch [7150/10000], Loss: 1.4076\n",
      "Epoch [7160/10000], Loss: 1.4075\n",
      "Epoch [7170/10000], Loss: 1.4075\n",
      "Epoch [7180/10000], Loss: 1.4074\n",
      "Epoch [7190/10000], Loss: 1.4074\n",
      "Epoch [7200/10000], Loss: 1.4073\n",
      "Epoch [7210/10000], Loss: 1.4073\n",
      "Epoch [7220/10000], Loss: 1.4072\n",
      "Epoch [7230/10000], Loss: 1.4071\n",
      "Epoch [7240/10000], Loss: 1.4071\n",
      "Epoch [7250/10000], Loss: 1.4070\n",
      "Epoch [7260/10000], Loss: 1.4070\n",
      "Epoch [7270/10000], Loss: 1.4069\n",
      "Epoch [7280/10000], Loss: 1.4069\n",
      "Epoch [7290/10000], Loss: 1.4068\n",
      "Epoch [7300/10000], Loss: 1.4068\n",
      "Epoch [7310/10000], Loss: 1.4067\n",
      "Epoch [7320/10000], Loss: 1.4066\n",
      "Epoch [7330/10000], Loss: 1.4066\n",
      "Epoch [7340/10000], Loss: 1.4065\n",
      "Epoch [7350/10000], Loss: 1.4065\n",
      "Epoch [7360/10000], Loss: 1.4064\n",
      "Epoch [7370/10000], Loss: 1.4064\n",
      "Epoch [7380/10000], Loss: 1.4063\n",
      "Epoch [7390/10000], Loss: 1.4063\n",
      "Epoch [7400/10000], Loss: 1.4062\n",
      "Epoch [7410/10000], Loss: 1.4062\n",
      "Epoch [7420/10000], Loss: 1.4061\n",
      "Epoch [7430/10000], Loss: 1.4061\n",
      "Epoch [7440/10000], Loss: 1.4060\n",
      "Epoch [7450/10000], Loss: 1.4059\n",
      "Epoch [7460/10000], Loss: 1.4059\n",
      "Epoch [7470/10000], Loss: 1.4058\n",
      "Epoch [7480/10000], Loss: 1.4058\n",
      "Epoch [7490/10000], Loss: 1.4057\n",
      "Epoch [7500/10000], Loss: 1.4057\n",
      "Epoch [7510/10000], Loss: 1.4056\n",
      "Epoch [7520/10000], Loss: 1.4056\n",
      "Epoch [7530/10000], Loss: 1.4055\n",
      "Epoch [7540/10000], Loss: 1.4055\n",
      "Epoch [7550/10000], Loss: 1.4054\n",
      "Epoch [7560/10000], Loss: 1.4054\n",
      "Epoch [7570/10000], Loss: 1.4053\n",
      "Epoch [7580/10000], Loss: 1.4053\n",
      "Epoch [7590/10000], Loss: 1.4052\n",
      "Epoch [7600/10000], Loss: 1.4052\n",
      "Epoch [7610/10000], Loss: 1.4051\n",
      "Epoch [7620/10000], Loss: 1.4051\n",
      "Epoch [7630/10000], Loss: 1.4050\n",
      "Epoch [7640/10000], Loss: 1.4050\n",
      "Epoch [7650/10000], Loss: 1.4049\n",
      "Epoch [7660/10000], Loss: 1.4049\n",
      "Epoch [7670/10000], Loss: 1.4049\n",
      "Epoch [7680/10000], Loss: 1.4048\n",
      "Epoch [7690/10000], Loss: 1.4048\n",
      "Epoch [7700/10000], Loss: 1.4047\n",
      "Epoch [7710/10000], Loss: 1.4047\n",
      "Epoch [7720/10000], Loss: 1.4046\n",
      "Epoch [7730/10000], Loss: 1.4046\n",
      "Epoch [7740/10000], Loss: 1.4046\n",
      "Epoch [7750/10000], Loss: 1.4045\n",
      "Epoch [7760/10000], Loss: 1.4044\n",
      "Epoch [7770/10000], Loss: 1.4043\n",
      "Epoch [7780/10000], Loss: 1.4042\n",
      "Epoch [7790/10000], Loss: 1.4041\n",
      "Epoch [7800/10000], Loss: 1.4041\n",
      "Epoch [7810/10000], Loss: 1.4040\n",
      "Epoch [7820/10000], Loss: 1.4040\n",
      "Epoch [7830/10000], Loss: 1.4039\n",
      "Epoch [7840/10000], Loss: 1.4039\n",
      "Epoch [7850/10000], Loss: 1.4038\n",
      "Epoch [7860/10000], Loss: 1.4038\n",
      "Epoch [7870/10000], Loss: 1.4037\n",
      "Epoch [7880/10000], Loss: 1.4037\n",
      "Epoch [7890/10000], Loss: 1.4037\n",
      "Epoch [7900/10000], Loss: 1.4036\n",
      "Epoch [7910/10000], Loss: 1.4036\n",
      "Epoch [7920/10000], Loss: 1.4035\n",
      "Epoch [7930/10000], Loss: 1.4035\n",
      "Epoch [7940/10000], Loss: 1.4035\n",
      "Epoch [7950/10000], Loss: 1.4034\n",
      "Epoch [7960/10000], Loss: 1.4034\n",
      "Epoch [7970/10000], Loss: 1.4033\n",
      "Epoch [7980/10000], Loss: 1.4033\n",
      "Epoch [7990/10000], Loss: 1.4033\n",
      "Epoch [8000/10000], Loss: 1.4032\n",
      "Epoch [8010/10000], Loss: 1.4032\n",
      "Epoch [8020/10000], Loss: 1.4031\n",
      "Epoch [8030/10000], Loss: 1.4031\n",
      "Epoch [8040/10000], Loss: 1.4031\n",
      "Epoch [8050/10000], Loss: 1.4030\n",
      "Epoch [8060/10000], Loss: 1.4030\n",
      "Epoch [8070/10000], Loss: 1.4029\n",
      "Epoch [8080/10000], Loss: 1.4029\n",
      "Epoch [8090/10000], Loss: 1.4029\n",
      "Epoch [8100/10000], Loss: 1.4028\n",
      "Epoch [8110/10000], Loss: 1.4028\n",
      "Epoch [8120/10000], Loss: 1.4028\n",
      "Epoch [8130/10000], Loss: 1.4027\n",
      "Epoch [8140/10000], Loss: 1.4027\n",
      "Epoch [8150/10000], Loss: 1.4027\n",
      "Epoch [8160/10000], Loss: 1.4026\n",
      "Epoch [8170/10000], Loss: 1.4026\n",
      "Epoch [8180/10000], Loss: 1.4026\n",
      "Epoch [8190/10000], Loss: 1.4025\n",
      "Epoch [8200/10000], Loss: 1.4025\n",
      "Epoch [8210/10000], Loss: 1.4025\n",
      "Epoch [8220/10000], Loss: 1.4024\n",
      "Epoch [8230/10000], Loss: 1.4024\n",
      "Epoch [8240/10000], Loss: 1.4024\n",
      "Epoch [8250/10000], Loss: 1.4023\n",
      "Epoch [8260/10000], Loss: 1.4023\n",
      "Epoch [8270/10000], Loss: 1.4023\n",
      "Epoch [8280/10000], Loss: 1.4023\n",
      "Epoch [8290/10000], Loss: 1.4022\n",
      "Epoch [8300/10000], Loss: 1.4022\n",
      "Epoch [8310/10000], Loss: 1.4022\n",
      "Epoch [8320/10000], Loss: 1.4021\n",
      "Epoch [8330/10000], Loss: 1.4021\n",
      "Epoch [8340/10000], Loss: 1.4021\n",
      "Epoch [8350/10000], Loss: 1.4020\n",
      "Epoch [8360/10000], Loss: 1.4020\n",
      "Epoch [8370/10000], Loss: 1.4020\n",
      "Epoch [8380/10000], Loss: 1.4020\n",
      "Epoch [8390/10000], Loss: 1.4019\n",
      "Epoch [8400/10000], Loss: 1.4019\n",
      "Epoch [8410/10000], Loss: 1.4019\n",
      "Epoch [8420/10000], Loss: 1.4018\n",
      "Epoch [8430/10000], Loss: 1.4018\n",
      "Epoch [8440/10000], Loss: 1.4018\n",
      "Epoch [8450/10000], Loss: 1.4017\n",
      "Epoch [8460/10000], Loss: 1.4017\n",
      "Epoch [8470/10000], Loss: 1.4017\n",
      "Epoch [8480/10000], Loss: 1.4017\n",
      "Epoch [8490/10000], Loss: 1.4016\n",
      "Epoch [8500/10000], Loss: 1.4016\n",
      "Epoch [8510/10000], Loss: 1.4016\n",
      "Epoch [8520/10000], Loss: 1.4016\n",
      "Epoch [8530/10000], Loss: 1.4015\n",
      "Epoch [8540/10000], Loss: 1.4015\n",
      "Epoch [8550/10000], Loss: 1.4015\n",
      "Epoch [8560/10000], Loss: 1.4014\n",
      "Epoch [8570/10000], Loss: 1.4014\n",
      "Epoch [8580/10000], Loss: 1.4014\n",
      "Epoch [8590/10000], Loss: 1.4014\n",
      "Epoch [8600/10000], Loss: 1.4013\n",
      "Epoch [8610/10000], Loss: 1.4013\n",
      "Epoch [8620/10000], Loss: 1.4013\n",
      "Epoch [8630/10000], Loss: 1.4013\n",
      "Epoch [8640/10000], Loss: 1.4012\n",
      "Epoch [8650/10000], Loss: 1.4012\n",
      "Epoch [8660/10000], Loss: 1.4012\n",
      "Epoch [8670/10000], Loss: 1.4012\n",
      "Epoch [8680/10000], Loss: 1.4011\n",
      "Epoch [8690/10000], Loss: 1.4011\n",
      "Epoch [8700/10000], Loss: 1.4011\n",
      "Epoch [8710/10000], Loss: 1.4011\n",
      "Epoch [8720/10000], Loss: 1.4010\n",
      "Epoch [8730/10000], Loss: 1.4010\n",
      "Epoch [8740/10000], Loss: 1.4010\n",
      "Epoch [8750/10000], Loss: 1.4010\n",
      "Epoch [8760/10000], Loss: 1.4009\n",
      "Epoch [8770/10000], Loss: 1.4009\n",
      "Epoch [8780/10000], Loss: 1.4009\n",
      "Epoch [8790/10000], Loss: 1.4009\n",
      "Epoch [8800/10000], Loss: 1.4008\n",
      "Epoch [8810/10000], Loss: 1.4008\n",
      "Epoch [8820/10000], Loss: 1.4008\n",
      "Epoch [8830/10000], Loss: 1.4008\n",
      "Epoch [8840/10000], Loss: 1.4007\n",
      "Epoch [8850/10000], Loss: 1.4007\n",
      "Epoch [8860/10000], Loss: 1.4007\n",
      "Epoch [8870/10000], Loss: 1.4007\n",
      "Epoch [8880/10000], Loss: 1.4006\n",
      "Epoch [8890/10000], Loss: 1.4006\n",
      "Epoch [8900/10000], Loss: 1.4006\n",
      "Epoch [8910/10000], Loss: 1.4005\n",
      "Epoch [8920/10000], Loss: 1.4005\n",
      "Epoch [8930/10000], Loss: 1.4005\n",
      "Epoch [8940/10000], Loss: 1.4005\n",
      "Epoch [8950/10000], Loss: 1.4005\n",
      "Epoch [8960/10000], Loss: 1.4004\n",
      "Epoch [8970/10000], Loss: 1.4004\n",
      "Epoch [8980/10000], Loss: 1.4004\n",
      "Epoch [8990/10000], Loss: 1.4004\n",
      "Epoch [9000/10000], Loss: 1.4003\n",
      "Epoch [9010/10000], Loss: 1.4003\n",
      "Epoch [9020/10000], Loss: 1.4003\n",
      "Epoch [9030/10000], Loss: 1.4003\n",
      "Epoch [9040/10000], Loss: 1.4002\n",
      "Epoch [9050/10000], Loss: 1.4002\n",
      "Epoch [9060/10000], Loss: 1.4002\n",
      "Epoch [9070/10000], Loss: 1.4002\n",
      "Epoch [9080/10000], Loss: 1.4002\n",
      "Epoch [9090/10000], Loss: 1.4001\n",
      "Epoch [9100/10000], Loss: 1.4001\n",
      "Epoch [9110/10000], Loss: 1.4001\n",
      "Epoch [9120/10000], Loss: 1.4001\n",
      "Epoch [9130/10000], Loss: 1.4000\n",
      "Epoch [9140/10000], Loss: 1.4000\n",
      "Epoch [9150/10000], Loss: 1.4000\n",
      "Epoch [9160/10000], Loss: 1.4000\n",
      "Epoch [9170/10000], Loss: 1.4000\n",
      "Epoch [9180/10000], Loss: 1.3999\n",
      "Epoch [9190/10000], Loss: 1.3999\n",
      "Epoch [9200/10000], Loss: 1.3999\n",
      "Epoch [9210/10000], Loss: 1.3999\n",
      "Epoch [9220/10000], Loss: 1.3999\n",
      "Epoch [9230/10000], Loss: 1.3998\n",
      "Epoch [9240/10000], Loss: 1.3998\n",
      "Epoch [9250/10000], Loss: 1.3998\n",
      "Epoch [9260/10000], Loss: 1.3998\n",
      "Epoch [9270/10000], Loss: 1.3997\n",
      "Epoch [9280/10000], Loss: 1.3997\n",
      "Epoch [9290/10000], Loss: 1.3997\n",
      "Epoch [9300/10000], Loss: 1.3997\n",
      "Epoch [9310/10000], Loss: 1.3997\n",
      "Epoch [9320/10000], Loss: 1.3996\n",
      "Epoch [9330/10000], Loss: 1.3996\n",
      "Epoch [9340/10000], Loss: 1.3996\n",
      "Epoch [9350/10000], Loss: 1.3996\n",
      "Epoch [9360/10000], Loss: 1.3996\n",
      "Epoch [9370/10000], Loss: 1.3995\n",
      "Epoch [9380/10000], Loss: 1.3995\n",
      "Epoch [9390/10000], Loss: 1.3995\n",
      "Epoch [9400/10000], Loss: 1.3995\n",
      "Epoch [9410/10000], Loss: 1.3995\n",
      "Epoch [9420/10000], Loss: 1.3994\n",
      "Epoch [9430/10000], Loss: 1.3994\n",
      "Epoch [9440/10000], Loss: 1.3994\n",
      "Epoch [9450/10000], Loss: 1.3994\n",
      "Epoch [9460/10000], Loss: 1.3994\n",
      "Epoch [9470/10000], Loss: 1.3993\n",
      "Epoch [9480/10000], Loss: 1.3993\n",
      "Epoch [9490/10000], Loss: 1.3993\n",
      "Epoch [9500/10000], Loss: 1.3993\n",
      "Epoch [9510/10000], Loss: 1.3992\n",
      "Epoch [9520/10000], Loss: 1.3992\n",
      "Epoch [9530/10000], Loss: 1.3992\n",
      "Epoch [9540/10000], Loss: 1.3992\n",
      "Epoch [9550/10000], Loss: 1.3992\n",
      "Epoch [9560/10000], Loss: 1.3991\n",
      "Epoch [9570/10000], Loss: 1.3991\n",
      "Epoch [9580/10000], Loss: 1.3991\n",
      "Epoch [9590/10000], Loss: 1.3991\n",
      "Epoch [9600/10000], Loss: 1.3991\n",
      "Epoch [9610/10000], Loss: 1.3991\n",
      "Epoch [9620/10000], Loss: 1.3990\n",
      "Epoch [9630/10000], Loss: 1.3990\n",
      "Epoch [9640/10000], Loss: 1.3990\n",
      "Epoch [9650/10000], Loss: 1.3990\n",
      "Epoch [9660/10000], Loss: 1.3990\n",
      "Epoch [9670/10000], Loss: 1.3989\n",
      "Epoch [9680/10000], Loss: 1.3989\n",
      "Epoch [9690/10000], Loss: 1.3989\n",
      "Epoch [9700/10000], Loss: 1.3989\n",
      "Epoch [9710/10000], Loss: 1.3989\n",
      "Epoch [9720/10000], Loss: 1.3988\n",
      "Epoch [9730/10000], Loss: 1.3988\n",
      "Epoch [9740/10000], Loss: 1.3988\n",
      "Epoch [9750/10000], Loss: 1.3988\n",
      "Epoch [9760/10000], Loss: 1.3988\n",
      "Epoch [9770/10000], Loss: 1.3987\n",
      "Epoch [9780/10000], Loss: 1.3987\n",
      "Epoch [9790/10000], Loss: 1.3987\n",
      "Epoch [9800/10000], Loss: 1.3987\n",
      "Epoch [9810/10000], Loss: 1.3987\n",
      "Epoch [9820/10000], Loss: 1.3986\n",
      "Epoch [9830/10000], Loss: 1.3986\n",
      "Epoch [9840/10000], Loss: 1.3986\n",
      "Epoch [9850/10000], Loss: 1.3986\n",
      "Epoch [9860/10000], Loss: 1.3986\n",
      "Epoch [9870/10000], Loss: 1.3985\n",
      "Epoch [9880/10000], Loss: 1.3985\n",
      "Epoch [9890/10000], Loss: 1.3985\n",
      "Epoch [9900/10000], Loss: 1.3985\n",
      "Epoch [9910/10000], Loss: 1.3985\n",
      "Epoch [9920/10000], Loss: 1.3985\n",
      "Epoch [9930/10000], Loss: 1.3984\n",
      "Epoch [9940/10000], Loss: 1.3984\n",
      "Epoch [9950/10000], Loss: 1.3984\n",
      "Epoch [9960/10000], Loss: 1.3984\n",
      "Epoch [9970/10000], Loss: 1.3984\n",
      "Epoch [9980/10000], Loss: 1.3983\n",
      "Epoch [9990/10000], Loss: 1.3983\n",
      "Epoch [10000/10000], Loss: 1.3983\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10000\n",
    "early_stop_patience = 10\n",
    "best_loss = float('inf')\n",
    "patience = 0\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = mlp_classifier_torch(X_train_class_tensor)\n",
    "    loss = criterion(outputs, y_train_class_tensor)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if loss.item() < best_loss:\n",
    "        best_loss = loss.item()\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "\n",
    "    if patience >= early_stop_patience:\n",
    "        print(f\"Stopping early at epoch {epoch + 1}\")\n",
    "        break\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00      1098\n",
      "           1       0.77      0.79      0.78      5867\n",
      "           2       0.66      0.62      0.64      2338\n",
      "           3       0.66      0.87      0.75      4697\n",
      "           4       0.81      0.81      0.81      1051\n",
      "           5       0.77      0.81      0.79      2391\n",
      "           6       0.96      0.84      0.90      4165\n",
      "\n",
      "    accuracy                           0.76     21607\n",
      "   macro avg       0.80      0.68      0.67     21607\n",
      "weighted avg       0.78      0.76      0.74     21607\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mlp_classifier = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=300, random_state=42)\n",
    "# mlp_classifier.fit(X_train_class, y_train_class)\n",
    "# y_pred_class_mlp = mlp_classifier_torch.predict(X_test_class)\n",
    "with torch.no_grad():\n",
    "    y_pred_class_mlp_logits = mlp_classifier_torch(X_test_class_tensor)\n",
    "    y_pred_class_mlp = torch.argmax(y_pred_class_mlp_logits, dim=1).cpu().numpy()\n",
    "\n",
    "print(\"MLP Classification Report:\")\n",
    "print(classification_report(y_test_class, y_pred_class_mlp, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPRegressorTorch(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MLPRegressorTorch, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 100)\n",
    "        self.fc2 = nn.Linear(100, 50)\n",
    "        self.fc3 = nn.Linear(50, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_regressor_torch = MLPRegressorTorch(input_size)\n",
    "mlp_regressor_torch = mlp_regressor_torch.to(device)\n",
    "\n",
    "criterion_reg = nn.MSELoss()\n",
    "criterion_reg = criterion_reg.to(device)\n",
    "optimizer_reg = optim.Adam(mlp_regressor_torch.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14149/4292142752.py:1: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14149/964189280.py:6: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/home/chyavanshenoy/Projects/Uni-Projects/AAI-500-Final-Project/.venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([86428])) that is different to the input size (torch.Size([86428, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 8\u001b[0m\n\u001b[1;32m      7\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m mlp_regressor_torch(X_train_reg_tensor)\n\u001b[0;32m----> 8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion_reg\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_reg_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m optimizer_reg\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/Projects/Uni-Projects/AAI-500-Final-Project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/Uni-Projects/AAI-500-Final-Project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/Uni-Projects/AAI-500-Final-Project/.venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:608\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/Uni-Projects/AAI-500-Final-Project/.venv/lib/python3.12/site-packages/torch/nn/functional.py:3792\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3791\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbroadcast_tensors(\u001b[38;5;28minput\u001b[39m, target)\n\u001b[0;32m-> 3792\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpanded_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpanded_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3794\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 27.83 GiB. GPU 0 has a total capacity of 4.00 GiB of which 3.04 GiB is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 100.08 MiB is allocated by PyTorch, and 53.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m patience \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m----> 6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mautocast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmlp_regressor_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_reg_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcriterion_reg\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_reg_tensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/Uni-Projects/AAI-500-Final-Project/.venv/lib/python3.12/site-packages/torch/cuda/amp/autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# TODO: discuss a unified TorchScript-friendly API for autocast\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type: Any, exc_val: Any, exc_tb: Any):  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_jit_internal\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10000\n",
    "early_stop_patience = 10\n",
    "best_loss = float('inf')\n",
    "patience = 0\n",
    "for epoch in range(num_epochs):\n",
    "    with autocast():\n",
    "        outputs = mlp_regressor_torch(X_train_reg_tensor)\n",
    "        loss = criterion_reg(outputs, y_train_reg_tensor)\n",
    "\n",
    "    optimizer_reg.zero_grad()\n",
    "    # loss.backward()\n",
    "    # optimizer_reg.step()\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer_reg)\n",
    "    scaler.update()\n",
    "\n",
    "    if loss.item() < best_loss:\n",
    "        best_loss = loss.item()\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "\n",
    "    if patience >= early_stop_patience:\n",
    "        print(f\"Stopping early at epoch {epoch + 1}\")\n",
    "        break\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Regression Metrics:\n",
      "MSE: 1699.397994992062\n",
      "R2 Score: 0.879401431440758\n"
     ]
    }
   ],
   "source": [
    "mlp_regressor = MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=300, random_state=42)\n",
    "mlp_regressor.fit(X_train_reg, y_train_reg)\n",
    "y_pred_reg_mlp = mlp_regressor.predict(X_test_reg)\n",
    "print(\"MLP Regression Metrics:\")\n",
    "print(\"MSE:\", mean_squared_error(y_test_reg, y_pred_reg_mlp))\n",
    "print(\"R2 Score:\", r2_score(y_test_reg, y_pred_reg_mlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Dual Prediction Metrics:\n",
      "MSE (AQI_Bucket): 1.987573014570189\n",
      "MSE (AQI): 1690.2473674677003\n"
     ]
    }
   ],
   "source": [
    "y_dual = df[[target_classification, target_regression]]\n",
    "X_train_dual, X_test_dual, y_train_dual, y_test_dual = train_test_split(X, y_dual, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_dual = scaler.fit_transform(X_train_dual)\n",
    "X_test_dual = scaler.transform(X_test_dual)\n",
    "\n",
    "mlp_dual = MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=300, random_state=42)\n",
    "mlp_dual.fit(X_train_dual, y_train_dual)\n",
    "y_pred_dual = mlp_dual.predict(X_test_dual)\n",
    "\n",
    "print(\"MLP Dual Prediction Metrics:\")\n",
    "print(\"MSE (AQI_Bucket):\", mean_squared_error(y_test_dual[target_classification], y_pred_dual[:, 0]))\n",
    "print(\"MSE (AQI):\", mean_squared_error(y_test_dual[target_regression], y_pred_dual[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label_encoder.pkl']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(logistic_model, 'logistic_model.pkl')\n",
    "joblib.dump(rf_classifier, 'rf_classifier.pkl')\n",
    "joblib.dump(rf_regressor, 'rf_regressor.pkl')\n",
    "joblib.dump(mlp_classifier, 'mlp_classifier.pkl')\n",
    "joblib.dump(mlp_regressor, 'mlp_regressor.pkl')\n",
    "joblib.dump(mlp_dual, 'mlp_dual.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "joblib.dump(label_encoder, 'label_encoder.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_models(x_new_test):\n",
    "    # Load models and scaler\n",
    "    logistic_model = joblib.load('logistic_model.pkl')\n",
    "    rf_classifier = joblib.load('rf_classifier.pkl')\n",
    "    rf_regressor = joblib.load('rf_regressor.pkl')\n",
    "    mlp_classifier = joblib.load('mlp_classifier.pkl')\n",
    "    mlp_regressor = joblib.load('mlp_regressor.pkl')\n",
    "    mlp_dual = joblib.load('mlp_dual.pkl')\n",
    "    scaler = joblib.load('scaler.pkl')\n",
    "    label_encoder = joblib.load('label_encoder.pkl')\n",
    "\n",
    "    # Scale the custom data\n",
    "    x_new_test_scaled = scaler.transform(x_new_test)\n",
    "\n",
    "    # Make predictions\n",
    "    logistic_pred = logistic_model.predict(x_new_test_scaled)\n",
    "    logistic_label = label_encoder.inverse_transform(logistic_pred)\n",
    "\n",
    "    rf_class_pred = rf_classifier.predict(x_new_test_scaled)\n",
    "    rf_class_label = label_encoder.inverse_transform(rf_class_pred)\n",
    "\n",
    "    rf_reg_pred = rf_regressor.predict(x_new_test_scaled)\n",
    "\n",
    "    mlp_class_pred = mlp_classifier.predict(x_new_test_scaled)\n",
    "    mlp_class_label = label_encoder.inverse_transform(mlp_class_pred)\n",
    "\n",
    "    mlp_reg_pred = mlp_regressor.predict(x_new_test_scaled)\n",
    "\n",
    "    mlp_dual_pred = mlp_dual.predict(x_new_test_scaled)\n",
    "    mlp_dual_class_label = label_encoder.inverse_transform(mlp_dual_pred[:, 0].astype(int))\n",
    "    mlp_dual_reg_pred = mlp_dual_pred[:, 1]\n",
    "\n",
    "    # Print predictions\n",
    "    print(\"Logistic Regression Prediction:\", logistic_label)\n",
    "    print(\"Random Forest Classification Prediction:\", rf_class_label)\n",
    "    print(\"Random Forest Regression Prediction:\", rf_reg_pred)\n",
    "    print(\"MLP Classification Prediction:\", mlp_class_label)\n",
    "    print(\"MLP Regression Prediction:\", mlp_reg_pred)\n",
    "    print(\"MLP Dual Prediction - Classification Label:\", mlp_dual_class_label)\n",
    "    print(\"MLP Dual Prediction - Regression Value:\", mlp_dual_reg_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_test_data = {\n",
    "    'PM2.5': [21],\n",
    "    'PM10': [19],\n",
    "    'NO2': [8],\n",
    "    'CO': [1],\n",
    "    'SO2': [1],\n",
    "    'O3': [10]\n",
    "}\n",
    "x_new_test = pd.DataFrame(custom_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Prediction: ['Satisfactory']\n",
      "Random Forest Classification Prediction: ['Satisfactory']\n",
      "Random Forest Regression Prediction: [80.76991485]\n",
      "MLP Classification Prediction: ['Satisfactory']\n",
      "MLP Regression Prediction: [66.30558873]\n",
      "MLP Dual Prediction - Classification Label: ['Moderate']\n",
      "MLP Dual Prediction - Regression Value: [64.0536822]\n"
     ]
    }
   ],
   "source": [
    "predict_models(x_new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
